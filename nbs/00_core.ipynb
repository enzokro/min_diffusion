{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core \n",
    "\n",
    "> Minimal pipeline for Diffusion Guidance experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from types import SimpleNamespace\n",
    "from fastcore.basics import store_attr\n",
    "# imports for diffusion models\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from diffusers import AutoencoderKL, UNet2DConditionModel\n",
    "from diffusers import LMSDiscreteScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# bring in all the helper function to process and plot latents\n",
    "from min_diffusion.utils import text_embeddings, image_from_latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOF while scanning triple-quoted string literal (2746884514.py, line 138)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [1]\u001b[0;36m\u001b[0m\n\u001b[0;31m    return latents\u001b[0m\n\u001b[0m                  \n^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOF while scanning triple-quoted string literal\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "\n",
    "class MinimalDiffusion:\n",
    "    \"\"\"Loads a Stable Diffusion pipeline.\n",
    "    \n",
    "    The goal is to have more control of the image generation loop. \n",
    "    This class loads the following individual pieces:\n",
    "        - Tokenizer\n",
    "        - Text encoder\n",
    "        - VAE\n",
    "        - U-Net\n",
    "        - Sampler\n",
    "        \n",
    "    The `self.generate` function uses these pieces to run a Diffusion image generation loop.\n",
    "    \n",
    "    This class can be subclasses and any of its methods overriden to gain even more control over the Diffusion pipeline. \n",
    "    \"\"\"\n",
    "    def __init__(self, model_name, device, dtype):\n",
    "        self.model_name = model_name\n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "        \n",
    "    def load_pipeline(self, unet_attn_slice=True,  better_vae=''):\n",
    "        \"\"\"Loads and returns the individual pieces in a Diffusion pipeline.        \n",
    "        \"\"\"\n",
    "        # load the pieces\n",
    "        self.load_text_pieces()\n",
    "        self.load_vae(better_vae)\n",
    "        self.load_unet(unet_attn_slice)\n",
    "        self.load_scheduler()\n",
    "        # put them on the device\n",
    "        self.to_device()\n",
    "    \n",
    "\n",
    "    def load_text_pieces(self):\n",
    "        \"\"\"Creates the tokenizer and text encoder.\n",
    "        \"\"\"\n",
    "        tokenizer = CLIPTokenizer.from_pretrained(\n",
    "            model_name,\n",
    "            subfolder=\"tokenizer\",\n",
    "            torch_dtype=self.dtype)\n",
    "        text_encoder = CLIPTextModel.from_pretrained(\n",
    "            model_name,\n",
    "            subfolder=\"text_encoder\",\n",
    "            torch_dtype=self.dtype)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.text_encoder = text_encoder\n",
    "    \n",
    "    \n",
    "    def load_vae(self, better_vae=''):\n",
    "        \"\"\"Loads the Variational Auto-Encoder.\n",
    "        \n",
    "        Optionally loads an improved `better_vae` from the stability.ai team.\n",
    "            It can be either the `ema` or `mse` VAE.\n",
    "        \"\"\"\n",
    "        # optionally use a VAE from stability that was trained for longer than the baseline \n",
    "        if better_vae:\n",
    "            assert better_vae in ('ema', 'mse')\n",
    "            print(f'Using the improved VAE \"{better_vae}\" from stabiliy.ai')\n",
    "            vae = AutoencoderKL.from_pretrained(\n",
    "                f\"stabilityai/sd-vae-ft-{better_vae}\",\n",
    "                torch_dtype=self.dtype)\n",
    "        else:\n",
    "            vae = AutoencoderKL.from_pretrained(model_name, subfolder='vae', torch_dtype=self.dtype)\n",
    "        self.vae = vae\n",
    "\n",
    "        \n",
    "    def load_unet(self, unet_attn_slice=True):\n",
    "        \"\"\"Loads the U-Net.\n",
    "        \n",
    "        Optionally uses attention slicing to fit on smaller GPU cards.\n",
    "        \"\"\"\n",
    "        unet = UNet2DConditionModel.from_pretrained(\n",
    "            model_name,\n",
    "            subfolder=\"unet\",\n",
    "            torch_dtype=self.dtype)\n",
    "        # optionally enable unet attention slicing\n",
    "        if unet_attn_slice:\n",
    "            print('Enabling default unet attention slicing.')\n",
    "            slice_size = unet.config.attention_head_dim // 2\n",
    "            unet.set_attention_slice(slice_size)\n",
    "        self.unet = unet\n",
    "        \n",
    "                \n",
    "    def load_scheduler(self):\n",
    "        \"\"\"Loads the scheduler.\n",
    "        \"\"\"\n",
    "        scheduler = LMSDiscreteScheduler.from_config(model_name, subfolder=\"scheduler\")\n",
    "        self.scheduler = scheduler \n",
    "\n",
    "        \n",
    "    def get_text_embeddings(self, text):\n",
    "        \"\"\"Embeds the given `text` prompt.\n",
    "        \"\"\"\n",
    "        return text_embeddings(text, self.tokenizer, self.text_encoder, device=self.device)\n",
    "    \n",
    "    def to_device(self, device=None):\n",
    "        \"\"\"Places to pipeline pieces on the given device\n",
    "        \n",
    "        Note: assumes we keep Scheduler and Tokenizer on the cpu.\n",
    "        \"\"\"\n",
    "        device = device or self.device\n",
    "        for m in (self.text_encoder, self.vae, self.unet):\n",
    "            m.to(device)\n",
    "\n",
    "    \n",
    "    def generate(\n",
    "        self,\n",
    "        prompt,\n",
    "        guide_tfm=None,\n",
    "        width=512,\n",
    "        height=512,\n",
    "        steps=50,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"Main image generation loop.\n",
    "        \"\"\"\n",
    "        # if no guidance transform was given, use the default update\n",
    "        if guide_tfm is None:\n",
    "            print('Using the default Classifier-free Guidance.')\n",
    "            G = 7.5\n",
    "            guide_tfm = lambda u, t, idx: u + G*(t - u)\n",
    "        self.guide_tfm = guide_tfm\n",
    "        \n",
    "        # prepare the text embeddings\n",
    "        text   = self.get_text_embeddings(prompt)\n",
    "        uncond = self.get_text_embeddings('')\n",
    "        text_emb = torch.cat([uncond, text]).type(self.unet.dtype)\n",
    "        \n",
    "        # start from the shared, initial latents\n",
    "        if not getattr(self, 'init_latents'):\n",
    "            self.init_latents = self.get_initial_latents(unet, height, width)\n",
    "            \n",
    "        latents = self.init_latents.clone()\n",
    "        self.scheduler.set_timesteps(steps)\n",
    "        latents = latents * self.scheduler.init_noise_sigma\n",
    "        \n",
    "        # run the diffusion process\n",
    "        for i,ts in enumerate(tqdm(self.scheduler.timesteps)):\n",
    "            latents = self.diffuse_step(latents, text_emb, ts, i)\n",
    "\n",
    "        # decode the final latents and return the generated image\n",
    "        image = image_from_latents(latents, self.vae)\n",
    "        return image    \n",
    "\n",
    "\n",
    "    def diffuse_step(self, latents, text_emb, ts, idx):\n",
    "        \"\"\"Runs a single diffusion step.\n",
    "        \"\"\"\n",
    "        inp = self.scheduler.scale_model_input(torch.cat([latents] * 2), ts)\n",
    "        with torch.no_grad(): \n",
    "            tf = ts\n",
    "            if torch.has_mps:\n",
    "                tf = ts.type(torch.float32)\n",
    "            u,t = self.unet(inp, tf, encoder_hidden_states=text_emb).sample.chunk(2)\n",
    "        \n",
    "        # run classifier-free guidance\n",
    "        pred = self.guide_tfm(u, t, idx)\n",
    "        \n",
    "        # update and return the latents\n",
    "        latents = self.scheduler.step(pred, ts, latents).prev_sample\n",
    "        return latents\n",
    "    \n",
    "    \n",
    "    def set_init_latents(self, latents):\n",
    "        \"\"\"Sets the given `latents` as the initial noise latents.\n",
    "        \"\"\"\n",
    "        self.init_latents = latents\n",
    "        \n",
    "        \n",
    "    def get_initial_latents(self, unet, height, width):\n",
    "        \"\"\"Returns \n",
    "        \"\"\"\n",
    "        return torch.randn((1, unet.in_channels, height//8, width//8), dtype=self.unet.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
