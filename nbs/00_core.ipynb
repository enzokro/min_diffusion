{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core \n",
    "\n",
    "> Minimal pipeline for Diffusion Guidance experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# imports for diffusion models\n",
    "from abc import ABC\n",
    "import importlib\n",
    "from PIL import Image\n",
    "import torch\n",
    "from tqdm.auto    import tqdm\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from diffusers    import AutoencoderKL, UNet2DConditionModel\n",
    "from diffusers    import LMSDiscreteScheduler, EulerDiscreteScheduler, DPMSolverMultistepScheduler, EulerAncestralDiscreteScheduler\n",
    "import torch\n",
    "from torch import nn\n",
    "try:\n",
    "    from k_diffusion.external import CompVisDenoiser, CompVisVDenoiser\n",
    "    from k_diffusion.sampling import get_sigmas_karras\n",
    "    import k_diffusion.sampling as k_sampling\n",
    "except:\n",
    "    print(f'WARNING: Could not import k_diffusion')\n",
    "from min_diffusion.kdiff import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "    \n",
    "class MinimalDiffusion:\n",
    "    \"\"\"Loads a Stable Diffusion pipeline.\n",
    "    \n",
    "    The goal is to have more control of the image generation loop. \n",
    "    This class loads the following individual pieces:\n",
    "        - Tokenizer\n",
    "        - Text encoder\n",
    "        - VAE\n",
    "        - U-Net\n",
    "        - Sampler\n",
    "        \n",
    "    The `self.generate` function uses these pieces to run a Diffusion image generation loop.\n",
    "    \n",
    "    This class can be subclasses and any of its methods overriden to gain even more control over the Diffusion pipeline. \n",
    "    \"\"\"\n",
    "    def __init__(self, model_name, device, dtype, revision, generator=None, use_k_diffusion='',\n",
    "                 better_vae='', unet_attn_slice=True, scheduler_kls='euler'):\n",
    "        self.model_name = model_name\n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "        self.revision = revision\n",
    "        self.generator = None \n",
    "        self.use_k_diffusion = use_k_diffusion\n",
    "        self.better_vae = better_vae\n",
    "        self.unet_attn_slice = unet_attn_slice\n",
    "        self.scheduler_kls = scheduler_kls\n",
    "        \n",
    "        \n",
    "    def load(self):\n",
    "        \"\"\"Loads and returns the individual pieces in a Diffusion pipeline.        \n",
    "        \"\"\"\n",
    "        # load the pieces\n",
    "        self.load_text_pieces()\n",
    "        self.load_vae()\n",
    "        self.load_unet()\n",
    "        self.load_scheduler()\n",
    "        # put them on the device\n",
    "        self.to_device()\n",
    "    \n",
    "    def load_text_pieces(self):\n",
    "        \"\"\"Creates the tokenizer and text encoder.\n",
    "        \"\"\"\n",
    "        tokenizer = CLIPTokenizer.from_pretrained(\n",
    "            self.model_name,\n",
    "            subfolder=\"tokenizer\",\n",
    "            torch_dtype=self.dtype)\n",
    "        text_encoder = CLIPTextModel.from_pretrained(\n",
    "            self.model_name,\n",
    "            subfolder=\"text_encoder\",\n",
    "            torch_dtype=self.dtype)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.text_encoder = text_encoder\n",
    "    \n",
    "    \n",
    "    def load_vae(self):\n",
    "        \"\"\"Loads the Variational Auto-Encoder.\n",
    "        \n",
    "        Optionally loads an improved `better_vae` from the stability.ai team.\n",
    "            It can be either the `ema` or `mse` VAE.\n",
    "        \"\"\"\n",
    "        # optionally use a VAE from stability that was trained for longer \n",
    "        if self.better_vae:\n",
    "            assert self.better_vae in ('ema', 'mse')\n",
    "            print(f'Using the improved VAE \"{better_vae}\" from stabiliy.ai')\n",
    "            vae = AutoencoderKL.from_pretrained(\n",
    "                f\"stabilityai/sd-vae-ft-{self.better_vae}\",\n",
    "                revision=self.revision,\n",
    "                torch_dtype=self.dtype)\n",
    "        else:\n",
    "            vae = AutoencoderKL.from_pretrained(self.model_name, subfolder='vae',\n",
    "                                                torch_dtype=self.dtype)\n",
    "        self.vae = vae\n",
    "\n",
    "        \n",
    "    def load_unet(self):\n",
    "        \"\"\"Loads the U-Net.\n",
    "        \n",
    "        Optionally uses attention slicing to fit on smaller GPU cards.\n",
    "        \"\"\"\n",
    "        unet = UNet2DConditionModel.from_pretrained(\n",
    "            self.model_name,\n",
    "            subfolder=\"unet\",\n",
    "            #revision=self.revision,\n",
    "            torch_dtype=self.dtype)\n",
    "        # optionally enable unet attention slicing\n",
    "        if self.unet_attn_slice:\n",
    "            print('Enabling default unet attention slicing.')\n",
    "            if isinstance(unet.config.attention_head_dim, int):\n",
    "                # half the attention head size is usually a good trade-off between\n",
    "                # speed and memory\n",
    "                slice_size = unet.config.attention_head_dim // 2\n",
    "            else:\n",
    "                # if `attention_head_dim` is a list, take the smallest head size\n",
    "                slice_size = min(unet.config.attention_head_dim)\n",
    "            unet.set_attention_slice(slice_size)\n",
    "        self.unet = unet\n",
    "        \n",
    "                \n",
    "    def load_scheduler(self):\n",
    "        \"\"\"Loads the scheduler.\n",
    "        \"\"\"\n",
    "        if self.scheduler_kls == 'euler':\n",
    "            sched_kls = EulerDiscreteScheduler\n",
    "        elif self.scheduler_kls == 'euler_ancestral':\n",
    "            sched_kls = EulerAncestralDiscreteScheduler\n",
    "        elif self.scheduler_kls == 'dpm_multi':\n",
    "            sched_kls = DPMSolverMultistepScheduler\n",
    "        else:\n",
    "            self.sched_kls = LMSDiscreteScheduler\n",
    "        print(f'Using scheduler: {sched_kls}')\n",
    "        if self.model_name.split('/')[-1] == 'stabilityai/stable-diffusion-2':\n",
    "            sched_kwargs = {'prediction_type': 'v-prediction'}\n",
    "        else:\n",
    "            sched_kwargs = {}\n",
    "        scheduler = sched_kls.from_pretrained(self.model_name, \n",
    "                                              subfolder=\"scheduler\",\n",
    "                                              **sched_kwargs)\n",
    "        self.scheduler = scheduler \n",
    "\n",
    "\n",
    "    def generate(\n",
    "        self,\n",
    "        prompt,\n",
    "        guide_tfm=None,\n",
    "        width=512,\n",
    "        height=512,\n",
    "        steps=50,\n",
    "        use_karras_sigmas=False,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"Main image generation loop.\n",
    "        \"\"\"\n",
    "        # if no guidance transform was given, use the default update\n",
    "        if guide_tfm is None:\n",
    "            print('NOTE: Using the default, static Classifier-free Guidance.')\n",
    "            G = 7.5\n",
    "            def guide_tfm(uncond, cond, idx): return uncod + G * (cond - uncond)\n",
    "        self.guide_tfm = guide_tfm\n",
    "        \n",
    "        # prepare the text embeddings\n",
    "        text = self.encode_text(prompt)\n",
    "        neg_prompt = kwargs.get('negative_prompt', '')\n",
    "        if neg_prompt:\n",
    "            print(f'Using negative prompt: {neg_prompt}')\n",
    "        uncond = self.encode_text(neg_prompt)\n",
    "        \n",
    "        # start from the shared, initial latents\n",
    "        if getattr(self, 'init_latents', None) is None:\n",
    "            self.init_latents = self.get_initial_latents(height, width)\n",
    "        latents = self.init_latents.clone().to(self.unet.device)\n",
    "        \n",
    "        # set the number of timesteps\n",
    "        # TODO: get alphas_cumprod from the k_diffusion schedules\n",
    "        self.scheduler.set_timesteps(steps, device=self.unet.device)\n",
    "        \n",
    "        if self.use_k_diffusion:\n",
    "            print(f'NOTE: Generating with k-diffusion Samplers')\n",
    "            \n",
    "            # load the sampler class\n",
    "            SamplerCls = SAMPLER_LOOKUP[self.use_k_diffusion]\n",
    "            model = ModelWrapper(self.unet, self.scheduler.alphas_cumprod)\n",
    "            sampler = SamplerCls(model, self.model_name)\n",
    "            \n",
    "            # set the positive and neutral (or negative) conditionings \n",
    "            positive_conditioning = text\n",
    "            neutral_conditioning = uncond\n",
    "            \n",
    "            # move wrapped sigmas and log-sigmas to device\n",
    "            sampler.cv_denoiser.sigmas = sampler.cv_denoiser.sigmas.to(latents.device)\n",
    "            sampler.cv_denoiser.log_sigmas = sampler.cv_denoiser.log_sigmas.to(latents.device)\n",
    "            \n",
    "            # sample with k_diffusion\n",
    "            latents = sampler.sample(\n",
    "                num_steps=steps,\n",
    "                initial_latent=latents,\n",
    "                positive_conditioning=positive_conditioning,\n",
    "                neutral_conditioning=neutral_conditioning,\n",
    "                t_start=None,#t_enc,\n",
    "                mask=None,#mask,\n",
    "                orig_latent=None,#init_latent,\n",
    "                shape=latents.shape,\n",
    "                batch_size=1,\n",
    "                guide_tfm=guide_tfm,\n",
    "                use_karras_sigmas=use_karras_sigmas,\n",
    "            )\n",
    "\n",
    "        \n",
    "        else:\n",
    "            # prepare the conditional and unconditional inputs\n",
    "            text_emb = torch.cat([uncond, text]).type(self.unet.dtype)\n",
    "            # scale the latents\n",
    "            latents = latents * self.scheduler.init_noise_sigma\n",
    "            # run the diffusion process\n",
    "            for i,ts in enumerate(tqdm(self.scheduler.timesteps)):\n",
    "                latents = self.diffuse_step(latents, text_emb, ts, i)\n",
    "\n",
    "        # decode the final latents and return the generated image\n",
    "        image = self.image_from_latents(latents)\n",
    "        return image    \n",
    "\n",
    "\n",
    "    def diffuse_step(self, latents, text_emb, ts, idx):\n",
    "        \"\"\"Runs a single diffusion step.\n",
    "        \"\"\"\n",
    "        inp = self.scheduler.scale_model_input(torch.cat([latents] * 2), ts)\n",
    "        with torch.no_grad(): \n",
    "            tf = ts\n",
    "            if torch.has_mps:\n",
    "                tf = ts.type(torch.float32)\n",
    "            preds = self.unet(inp, tf, encoder_hidden_states=text_emb)\n",
    "            u, t  = preds.sample.chunk(2)\n",
    "        \n",
    "        # run classifier-free guidance\n",
    "        pred = self.guide_tfm(u, t, idx)\n",
    "        \n",
    "        # update and return the latents\n",
    "        latents = self.scheduler.step(pred, ts, latents).prev_sample\n",
    "        return latents\n",
    "    \n",
    "\n",
    "    def encode_text(self, prompts, maxlen=None):\n",
    "        \"\"\"Extracts text embeddings from the given `prompts`.\n",
    "        \"\"\"\n",
    "        maxlen = maxlen or self.tokenizer.model_max_length\n",
    "        inp = self.tokenizer(prompts, padding=\"max_length\", max_length=maxlen, \n",
    "                             truncation=True, return_tensors=\"pt\")\n",
    "        inp_ids = inp.input_ids.to(self.device)\n",
    "        return self.text_encoder(inp_ids)[0]\n",
    "\n",
    "    \n",
    "    def to_device(self, device=None):\n",
    "        \"\"\"Places to pipeline pieces on the given device\n",
    "        \n",
    "        Note: assumes we keep Scheduler and Tokenizer on the cpu.\n",
    "        \"\"\"\n",
    "        device = device or self.device\n",
    "        for m in (self.text_encoder, self.vae, self.unet):\n",
    "            m.to(device)\n",
    "    \n",
    "    \n",
    "    def set_initial_latents(self, latents):\n",
    "        \"\"\"Sets the given `latents` as the initial noise latents.\n",
    "        \"\"\"\n",
    "        self.init_latents = latents\n",
    "        \n",
    "        \n",
    "    def get_initial_latents(self, height, width):\n",
    "        \"\"\"Returns an initial set of latents.\n",
    "        \"\"\"\n",
    "        return torch.randn((1, self.unet.in_channels, height//8, width//8),\n",
    "                           dtype=self.dtype, generator=self.generator)\n",
    "    \n",
    "    \n",
    "    def image_from_latents(self, latents):\n",
    "        \"\"\"Scales diffusion `latents` and turns them into a PIL Image.\n",
    "        \"\"\"\n",
    "        # scale and decode the latents\n",
    "        latents = 1 / 0.18215 * latents\n",
    "        with torch.no_grad():\n",
    "            data = self.vae.decode(latents.type(self.vae.dtype)).sample[0]\n",
    "        # Create PIL image\n",
    "        data = (data / 2 + 0.5).clamp(0, 1)\n",
    "        data = data.cpu().permute(1, 2, 0).float().numpy()\n",
    "        data = (data * 255).round().astype(\"uint8\")\n",
    "        image = Image.fromarray(data)\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| export\n",
    "\n",
    "# def get_device() -> str:\n",
    "#     \"\"\"Return the best torch backend available\"\"\"\n",
    "#     if torch.cuda.is_available():\n",
    "#         return \"cuda\"\n",
    "\n",
    "#     if torch.backends.mps.is_available():\n",
    "#         return \"mps:0\"\n",
    "\n",
    "#     return \"cpu\"\n",
    "\n",
    "# def ensure_4_dim(t: torch.Tensor):\n",
    "#     if len(t.shape) == 3:\n",
    "#         t = t.unsqueeze(dim=0)\n",
    "#     return t\n",
    "\n",
    "# class SamplerName:\n",
    "#     PLMS = \"plms\"\n",
    "#     DDIM = \"ddim\"\n",
    "#     K_DPM_FAST = \"k_dpm_fast\"\n",
    "#     K_DPM_ADAPTIVE = \"k_dpm_adaptive\"\n",
    "#     K_LMS = \"k_lms\"\n",
    "#     K_DPM_2 = \"k_dpm_2\"\n",
    "#     K_DPM_2_ANCESTRAL = \"k_dpm_2_a\"\n",
    "#     K_DPMPP_2M = \"k_dpmpp_2m\"\n",
    "#     K_DPMPP_2S_ANCESTRAL = \"k_dpmpp_2s_a\"\n",
    "#     K_EULER = \"k_euler\"\n",
    "#     K_EULER_ANCESTRAL = \"k_euler_a\"\n",
    "#     K_HEUN = \"k_heun\"\n",
    "#     K_DPMPP_SDE = 'k_dpmpp_sde'\n",
    "\n",
    "\n",
    "# class ImageSampler(ABC):\n",
    "#     short_name: str\n",
    "#     name: str\n",
    "#     default_steps: int\n",
    "#     default_size: int\n",
    "\n",
    "#     def __init__(self, model):\n",
    "#         self.model = model\n",
    "#         self.device = get_device()\n",
    "        \n",
    "        \n",
    "# def get_noise_prediction(\n",
    "#     denoise_func,\n",
    "#     noisy_latent,\n",
    "#     time_encoding,\n",
    "#     neutral_conditioning,\n",
    "#     positive_conditioning,\n",
    "#     signal_amplification=7.5,\n",
    "# ):\n",
    "#     noisy_latent = ensure_4_dim(noisy_latent)\n",
    "\n",
    "#     noisy_latent_in = torch.cat([noisy_latent] * 2)\n",
    "#     time_encoding_in = torch.cat([time_encoding] * 2)\n",
    "#     if isinstance(positive_conditioning, dict):\n",
    "#         assert isinstance(neutral_conditioning, dict)\n",
    "#         conditioning_in = {}\n",
    "#         for k in positive_conditioning:\n",
    "#             if isinstance(positive_conditioning[k], list):\n",
    "#                 conditioning_in[k] = [\n",
    "#                     torch.cat([neutral_conditioning[k][i], positive_conditioning[k][i]])\n",
    "#                     for i in range(len(positive_conditioning[k]))\n",
    "#                 ]\n",
    "#             else:\n",
    "#                 conditioning_in[k] = torch.cat(\n",
    "#                     [neutral_conditioning[k], positive_conditioning[k]]\n",
    "#                 )\n",
    "#     else:\n",
    "#         conditioning_in = torch.cat([neutral_conditioning, positive_conditioning])\n",
    "\n",
    "#     # the k-diffusion samplers actually return the denoised predicted latents but things seem\n",
    "#     # to work anyway\n",
    "#     noise_pred_neutral, noise_pred_positive = denoise_func(\n",
    "#         noisy_latent_in, time_encoding_in, conditioning_in\n",
    "#     ).chunk(2)\n",
    "\n",
    "#     amplified_noise_pred = signal_amplification * (\n",
    "#         noise_pred_positive - noise_pred_neutral\n",
    "#     )\n",
    "#     noise_pred = noise_pred_neutral + amplified_noise_pred\n",
    "\n",
    "#     return noise_pred\n",
    "\n",
    "\n",
    "# def mask_blend(noisy_latent, orig_latent, mask, mask_noise, ts, model):\n",
    "#     \"\"\"\n",
    "#     Apply a mask to the noisy_latent.\n",
    "#     ts is a decreasing value between 1000 and 1\n",
    "#     \"\"\"\n",
    "#     assert orig_latent is not None\n",
    "#     log_latent(orig_latent, \"orig_latent\")\n",
    "#     noised_orig_latent = model.q_sample(orig_latent, ts, mask_noise)\n",
    "\n",
    "#     # this helps prevent the weird disjointed images that can happen with masking\n",
    "#     hint_strength = 1\n",
    "#     # if we're in the first 10% of the steps then don't fully noise the parts\n",
    "#     # of the image we're not changing so that the algorithm can learn from the context\n",
    "#     if ts > 1000:\n",
    "#         hinted_orig_latent = (\n",
    "#             noised_orig_latent * (1 - hint_strength) + orig_latent * hint_strength\n",
    "#         )\n",
    "#         log_latent(hinted_orig_latent, f\"hinted_orig_latent {ts}\")\n",
    "#     else:\n",
    "#         hinted_orig_latent = noised_orig_latent\n",
    "\n",
    "#     hinted_orig_latent_masked = hinted_orig_latent * mask\n",
    "#     log_latent(hinted_orig_latent_masked, f\"hinted_orig_latent_masked {ts}\")\n",
    "#     noisy_latent_masked = (1.0 - mask) * noisy_latent\n",
    "#     log_latent(noisy_latent_masked, f\"noisy_latent_masked {ts}\")\n",
    "#     noisy_latent = hinted_orig_latent_masked + noisy_latent_masked\n",
    "#     log_latent(noisy_latent, f\"mask-blended noisy_latent {ts}\")\n",
    "#     return noisy_latent\n",
    "\n",
    "\n",
    "# #| export\n",
    "\n",
    "# class StandardCompVisDenoiser(CompVisDenoiser):\n",
    "#     def apply_model(self, *args, **kwargs):\n",
    "#         return self.inner_model.apply_model(*args, **kwargs)\n",
    "\n",
    "\n",
    "# class StandardCompVisVDenoiser(CompVisVDenoiser):\n",
    "#     def apply_model(self, *args, **kwargs):\n",
    "#         return self.inner_model.apply_model(*args, **kwargs)\n",
    "\n",
    "# #| export\n",
    "\n",
    "# def sample_dpm_adaptive(\n",
    "#     model, x, sigmas, extra_args=None, disable=False, callback=None\n",
    "# ):\n",
    "#     sigma_min = sigmas[-2]\n",
    "#     sigma_max = sigmas[0]\n",
    "#     return k_sampling.sample_dpm_adaptive(\n",
    "#         model=model,\n",
    "#         x=x,\n",
    "#         sigma_min=sigma_min,\n",
    "#         sigma_max=sigma_max,\n",
    "#         extra_args=extra_args,\n",
    "#         disable=disable,\n",
    "#         callback=callback,\n",
    "#     )\n",
    "\n",
    "\n",
    "# def sample_dpm_fast(model, x, sigmas, extra_args=None, disable=False, callback=None):\n",
    "#     sigma_min = sigmas[-2]\n",
    "#     sigma_max = sigmas[0]\n",
    "#     return k_sampling.sample_dpm_fast(\n",
    "#         model=model,\n",
    "#         x=x,\n",
    "#         sigma_min=sigma_min,\n",
    "#         sigma_max=sigma_max,\n",
    "#         n=len(sigmas),\n",
    "#         extra_args=extra_args,\n",
    "#         disable=disable,\n",
    "#         callback=callback,\n",
    "#     )\n",
    "\n",
    "\n",
    "# class KDiffusionSampler(ImageSampler, ABC):\n",
    "#     sampler_func: callable\n",
    "\n",
    "#     def __init__(self, model):\n",
    "#         super().__init__(model)\n",
    "#         denoiseer_cls = (\n",
    "#             StandardCompVisVDenoiser\n",
    "#             # if model.parameterization == \"v\"\n",
    "#             # else StandardCompVisDenoiser\n",
    "#         )\n",
    "#         self.cv_denoiser = denoiseer_cls(model)\n",
    "\n",
    "#     def sample(\n",
    "#         self,\n",
    "#         num_steps,\n",
    "#         shape,\n",
    "#         neutral_conditioning,\n",
    "#         positive_conditioning,\n",
    "#         guidance_scale,\n",
    "#         batch_size=1,\n",
    "#         mask=None,\n",
    "#         orig_latent=None,\n",
    "#         initial_latent=None,\n",
    "#         t_start=None,\n",
    "#     ):\n",
    "#         # if positive_conditioning.shape[0] != batch_size:\n",
    "#         #     raise ValueError(\n",
    "#         #         f\"Got {positive_conditioning.shape[0]} conditionings but batch-size is {batch_size}\"\n",
    "#         #     )\n",
    "\n",
    "#         if initial_latent is None:\n",
    "#             initial_latent = torch.randn(shape, device=\"cpu\").to(self.device)\n",
    "\n",
    "#         #log_latent(initial_latent, \"initial_latent\")\n",
    "#         if t_start is not None:\n",
    "#             t_start = num_steps - t_start + 1\n",
    "\n",
    "#         sigmas = self.cv_denoiser.get_sigmas(num_steps)[t_start:]\n",
    "\n",
    "#         # if our number of steps is zero, just return the initial latent\n",
    "#         if sigmas.nelement() == 0:\n",
    "#             if orig_latent is not None:\n",
    "#                 return orig_latent\n",
    "#             return initial_latent\n",
    "\n",
    "#         x = initial_latent * sigmas[0]\n",
    "#         #log_latent(x, \"initial_sigma_noised_tensor\")\n",
    "#         model_wrap_cfg = CFGDenoiser(self.cv_denoiser)\n",
    "\n",
    "#         mask_noise = None\n",
    "#         if mask is not None:\n",
    "#             mask_noise = torch.randn_like(initial_latent, device=\"cpu\").to(\n",
    "#                 initial_latent.device\n",
    "#             )\n",
    "\n",
    "#         samples = self.sampler_func(\n",
    "#             model=model_wrap_cfg,\n",
    "#             x=x,\n",
    "#             sigmas=sigmas,\n",
    "#             extra_args={\n",
    "#                 \"cond\": positive_conditioning,\n",
    "#                 \"uncond\": neutral_conditioning,\n",
    "#                 \"cond_scale\": guidance_scale,\n",
    "#                 \"mask\": mask,\n",
    "#                 \"mask_noise\": mask_noise,\n",
    "#                 \"orig_latent\": orig_latent,\n",
    "#             },\n",
    "#             disable=False,\n",
    "#             #callback=callback,\n",
    "#         )\n",
    "\n",
    "#         return samples\n",
    "\n",
    "\n",
    "# class DPMFastSampler(KDiffusionSampler):\n",
    "#     short_name = SamplerName.K_DPM_FAST\n",
    "#     name = \"Diffusion probabilistic models - fast\"\n",
    "#     default_steps = 15\n",
    "#     sampler_func = staticmethod(sample_dpm_fast)\n",
    "\n",
    "\n",
    "# class DPMAdaptiveSampler(KDiffusionSampler):\n",
    "#     short_name = SamplerName.K_DPM_ADAPTIVE\n",
    "#     name = \"Diffusion probabilistic models - adaptive\"\n",
    "#     default_steps = 40\n",
    "#     sampler_func = staticmethod(sample_dpm_adaptive)\n",
    "\n",
    "\n",
    "# class DPM2Sampler(KDiffusionSampler):\n",
    "#     short_name = SamplerName.K_DPM_2\n",
    "#     name = \"Diffusion probabilistic models - 2\"\n",
    "#     default_steps = 40\n",
    "#     sampler_func = staticmethod(k_sampling.sample_dpm_2)\n",
    "\n",
    "\n",
    "# class DPM2AncestralSampler(KDiffusionSampler):\n",
    "#     short_name = SamplerName.K_DPM_2_ANCESTRAL\n",
    "#     name = \"Diffusion probabilistic models - 2 ancestral\"\n",
    "#     default_steps = 40\n",
    "#     sampler_func = staticmethod(k_sampling.sample_dpm_2_ancestral)\n",
    "\n",
    "\n",
    "# class DPMPP2MSampler(KDiffusionSampler):\n",
    "#     short_name = SamplerName.K_DPMPP_2M\n",
    "#     name = \"Diffusion probabilistic models - 2m\"\n",
    "#     default_steps = 15\n",
    "#     sampler_func = staticmethod(k_sampling.sample_dpmpp_2m)\n",
    "    \n",
    "    \n",
    "# class DPMPPSDESampler(KDiffusionSampler):\n",
    "#     short_name = SamplerName.K_DPMPP_SDE\n",
    "#     name = \"Diffusion probabilistic models - 2m\"\n",
    "#     default_steps = 30\n",
    "#     sampler_func = staticmethod(k_sampling.sample_dpmpp_sde)\n",
    "\n",
    "\n",
    "# class DPMPP2SAncestralSampler(KDiffusionSampler):\n",
    "#     short_name = SamplerName.K_DPMPP_2S_ANCESTRAL\n",
    "#     name = \"Ancestral sampling with DPM-Solver++(2S) second-order steps.\"\n",
    "#     default_steps = 15\n",
    "#     sampler_func = staticmethod(k_sampling.sample_dpmpp_2s_ancestral)\n",
    "\n",
    "\n",
    "# class EulerSampler(KDiffusionSampler):\n",
    "#     short_name = SamplerName.K_EULER\n",
    "#     name = \"Algorithm 2 (Euler steps) from Karras et al. (2022)\"\n",
    "#     default_steps = 40\n",
    "#     sampler_func = staticmethod(k_sampling.sample_euler)\n",
    "\n",
    "\n",
    "# class EulerAncestralSampler(KDiffusionSampler):\n",
    "#     short_name = SamplerName.K_EULER_ANCESTRAL\n",
    "#     name = \"Euler ancestral\"\n",
    "#     default_steps = 40\n",
    "#     sampler_func = staticmethod(k_sampling.sample_euler_ancestral)\n",
    "\n",
    "\n",
    "# class HeunSampler(KDiffusionSampler):\n",
    "#     short_name = SamplerName.K_HEUN\n",
    "#     name = \"Algorithm 2 (Heun steps) from Karras et al. (2022).\"\n",
    "#     default_steps = 40\n",
    "#     sampler_func = staticmethod(k_sampling.sample_heun)\n",
    "\n",
    "\n",
    "# class LMSSampler(KDiffusionSampler):\n",
    "#     short_name = SamplerName.K_LMS\n",
    "#     name = \"LMS\"\n",
    "#     default_steps = 40\n",
    "#     sampler_func = staticmethod(k_sampling.sample_lms)\n",
    "\n",
    "\n",
    "# class CFGDenoiser(nn.Module):\n",
    "#     \"\"\"\n",
    "#     Conditional forward guidance wrapper\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self, model):\n",
    "#         super().__init__()\n",
    "#         self.inner_model = model\n",
    "#         self.device = get_device()\n",
    "\n",
    "#     def forward(\n",
    "#         self,\n",
    "#         x,\n",
    "#         sigma,\n",
    "#         uncond,\n",
    "#         cond,\n",
    "#         cond_scale,\n",
    "#         mask=None,\n",
    "#         mask_noise=None,\n",
    "#         orig_latent=None,\n",
    "#     ):\n",
    "#         def _wrapper(noisy_latent_in, time_encoding_in, conditioning_in):\n",
    "#             return self.inner_model(\n",
    "#                 noisy_latent_in, time_encoding_in, cond=conditioning_in\n",
    "#             )\n",
    "\n",
    "#         if mask is not None:\n",
    "#             assert orig_latent is not None\n",
    "#             t = self.inner_model.sigma_to_t(sigma, quantize=True).to(self.device)\n",
    "#             big_sigma = max(sigma, 1)\n",
    "#             x = mask_blend(\n",
    "#                 noisy_latent=x,\n",
    "#                 orig_latent=orig_latent * big_sigma,\n",
    "#                 mask=mask,\n",
    "#                 mask_noise=mask_noise * big_sigma,\n",
    "#                 ts=t,\n",
    "#                 model=self.inner_model.inner_model,\n",
    "#             )\n",
    "\n",
    "#         noise_pred = get_noise_prediction(\n",
    "#             denoise_func=_wrapper,\n",
    "#             noisy_latent=x,\n",
    "#             time_encoding=sigma,\n",
    "#             neutral_conditioning=uncond,\n",
    "#             positive_conditioning=cond,\n",
    "#             signal_amplification=cond_scale,\n",
    "#         )\n",
    "\n",
    "#         return noise_pred\n",
    "    \n",
    "    \n",
    "# SAMPLERS = [\n",
    "#     # PLMSSampler,\n",
    "#     # DDIMSampler,\n",
    "#     DPMFastSampler,\n",
    "#     DPMAdaptiveSampler,\n",
    "#     LMSSampler,\n",
    "#     DPM2Sampler,\n",
    "#     DPM2AncestralSampler,\n",
    "#     DPMPP2MSampler,\n",
    "#     DPMPPSDESampler,\n",
    "#     DPMPP2SAncestralSampler,\n",
    "#     EulerSampler,\n",
    "#     EulerAncestralSampler,\n",
    "#     HeunSampler,\n",
    "# ]\n",
    "\n",
    "# SAMPLER_LOOKUP = {sampler.short_name: sampler for sampler in SAMPLERS}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
