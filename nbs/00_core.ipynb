{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core \n",
    "\n",
    "> Minimal pipeline for Diffusion components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from types import SimpleNamespace\n",
    "from fastcore.basics import store_attr\n",
    "# imports for diffusion models\n",
    "import torch\n",
    "from tqdm.auto import tqdm \n",
    "from transformers import logging\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from huggingface_hub import notebook_login\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from diffusers import AutoencoderKL, UNet2DConditionModel\n",
    "from diffusers import LMSDiscreteScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# bring in all the helper function to process and plot latents\n",
    "from min_diffusion.utils import text_embeddings, image_from_latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class MinimalDiffusion:\n",
    "    def __init__(self, model_name, device, dtype):\n",
    "        store_attr()\n",
    "\n",
    "    def load_sd_pieces(\n",
    "        self,\n",
    "        attention_slicing=False,\n",
    "        better_vae=''):\n",
    "        \"Loads and returns the individual pieces in a Diffusion pipeline.\"\n",
    "        model_name = self.model_name\n",
    "        dtype = self.dtype\n",
    "        \n",
    "        # create the tokenizer and text encoder\n",
    "        tokenizer = CLIPTokenizer.from_pretrained(\n",
    "            model_name,\n",
    "            subfolder=\"tokenizer\",\n",
    "            torch_dtype=dtype)\n",
    "        text_encoder = CLIPTextModel.from_pretrained(\n",
    "            model_name,\n",
    "            subfolder=\"text_encoder\",\n",
    "            torch_dtype=dtype).to(self.device)\n",
    "\n",
    "        # we are using a VAE from stability that was trained for longer than the baseline \n",
    "        if better_vae:\n",
    "            assert better_vae in ('ema', 'mse')\n",
    "            vae = AutoencoderKL.from_pretrained(f\"stabilityai/sd-vae-ft-{better_vae}\", torch_dtype=dtype).to(self.device)\n",
    "        else:\n",
    "            vae = AutoencoderKL.from_pretrained(model_name, subfolder='vae', torch_dtype=dtype).to(self.device)\n",
    "        \n",
    "        # build the unet\n",
    "        unet = UNet2DConditionModel.from_pretrained(\n",
    "            model_name,\n",
    "            subfolder=\"unet\",\n",
    "            torch_dtype=dtype).to(self.device)\n",
    "        \n",
    "        # enable unet attention slicing\n",
    "        if attention_slicing:\n",
    "            print('Enabling default unet attention slicing.')\n",
    "            slice_size = unet.config.attention_head_dim // 2\n",
    "            unet.set_attention_slice(slice_size)\n",
    "            \n",
    "        # build the scheduler\n",
    "        scheduler = LMSDiscreteScheduler.from_config(model_name, subfolder=\"scheduler\")\n",
    "        \n",
    "        # create and return a simple pipeline with all of the pieces\n",
    "        self.tokenizer = tokenizer\n",
    "        self.text_encoder = text_encoder\n",
    "        self.vae = vae\n",
    "        self.unet = unet\n",
    "        self.scheduler = scheduler\n",
    "\n",
    "    def set_init_latents(self, latents):\n",
    "        self.init_latents = latents\n",
    "\n",
    "    def generate(\n",
    "        self,\n",
    "        prompt,\n",
    "        guide_tfm=None,\n",
    "        width=512,\n",
    "        height=512,\n",
    "        steps=50,\n",
    "        **kwargs):\n",
    "        # make sure we have a guidance transformation\n",
    "        assert guide_tfm\n",
    "        \n",
    "        # prepare the text embeddings\n",
    "        text = text_embeddings(prompt, self.tokenizer, self.text_encoder, device=self.device)\n",
    "        uncond = text_embeddings('', self.tokenizer, self.text_encoder, device=self.device)\n",
    "        emb = torch.cat([uncond, text]).type(self.unet.dtype)\n",
    "        \n",
    "        # start from the shared, initial latents\n",
    "        latents = torch.clone(self.init_latents)\n",
    "        self.scheduler.set_timesteps(steps)\n",
    "        latents = latents * self.scheduler.init_noise_sigma\n",
    "        \n",
    "        # run the diffusion process\n",
    "        for i,ts in enumerate(tqdm(self.scheduler.timesteps)):\n",
    "            inp = self.scheduler.scale_model_input(torch.cat([latents] * 2), ts)\n",
    "            with torch.no_grad(): \n",
    "                tf = ts\n",
    "                if torch.has_mps:\n",
    "                    tf = ts.type(torch.float32)\n",
    "                u,t = self.unet(inp, tf, encoder_hidden_states=emb).sample.chunk(2)\n",
    "            \n",
    "            # call the guidance transform\n",
    "            pred = guide_tfm(u, t, idx=i)\n",
    "            \n",
    "            # update the latents\n",
    "            latents = self.scheduler.step(pred, ts, latents).prev_sample\n",
    "            \n",
    "        # decode the final latents and return the generated image\n",
    "        image = image_from_latents(latents, self.vae)\n",
    "        return image    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('base': conda)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
