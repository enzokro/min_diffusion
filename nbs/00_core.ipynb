{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core \n",
    "\n",
    "> Minimal pipeline for Diffusion Guidance experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# imports for diffusion models\n",
    "from PIL import Image\n",
    "import torch\n",
    "from tqdm.auto    import tqdm\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from diffusers    import AutoencoderKL, UNet2DConditionModel\n",
    "from diffusers    import LMSDiscreteScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class MinimalDiffusion:\n",
    "    \"\"\"Loads a Stable Diffusion pipeline.\n",
    "    \n",
    "    The goal is to have more control of the image generation loop. \n",
    "    This class loads the following individual pieces:\n",
    "        - Tokenizer\n",
    "        - Text encoder\n",
    "        - VAE\n",
    "        - U-Net\n",
    "        - Sampler\n",
    "        \n",
    "    The `self.generate` function uses these pieces to run a Diffusion image generation loop.\n",
    "    \n",
    "    This class can be subclasses and any of its methods overriden to gain even more control over the Diffusion pipeline. \n",
    "    \"\"\"\n",
    "    def __init__(self, model_name, device, dtype, generator=None):\n",
    "        self.model_name = model_name\n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "        self.generator = None \n",
    "        \n",
    "        \n",
    "    def load(self, unet_attn_slice=True,  better_vae=''):\n",
    "        \"\"\"Loads and returns the individual pieces in a Diffusion pipeline.        \n",
    "        \"\"\"\n",
    "        # load the pieces\n",
    "        self.load_text_pieces()\n",
    "        self.load_vae(better_vae)\n",
    "        self.load_unet(unet_attn_slice)\n",
    "        self.load_scheduler()\n",
    "        # put them on the device\n",
    "        self.to_device()\n",
    "    \n",
    "\n",
    "    def load_text_pieces(self):\n",
    "        \"\"\"Creates the tokenizer and text encoder.\n",
    "        \"\"\"\n",
    "        tokenizer = CLIPTokenizer.from_pretrained(\n",
    "            self.model_name,\n",
    "            subfolder=\"tokenizer\",\n",
    "            torch_dtype=self.dtype)\n",
    "        text_encoder = CLIPTextModel.from_pretrained(\n",
    "            self.model_name,\n",
    "            subfolder=\"text_encoder\",\n",
    "            torch_dtype=self.dtype)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.text_encoder = text_encoder\n",
    "    \n",
    "    \n",
    "    def load_vae(self, better_vae=''):\n",
    "        \"\"\"Loads the Variational Auto-Encoder.\n",
    "        \n",
    "        Optionally loads an improved `better_vae` from the stability.ai team.\n",
    "            It can be either the `ema` or `mse` VAE.\n",
    "        \"\"\"\n",
    "        # optionally use a VAE from stability that was trained for longer \n",
    "        if better_vae:\n",
    "            assert better_vae in ('ema', 'mse')\n",
    "            print(f'Using the improved VAE \"{better_vae}\" from stabiliy.ai')\n",
    "            vae = AutoencoderKL.from_pretrained(\n",
    "                f\"stabilityai/sd-vae-ft-{better_vae}\",\n",
    "                torch_dtype=self.dtype)\n",
    "        else:\n",
    "            vae = AutoencoderKL.from_pretrained(self.model_name, subfolder='vae',\n",
    "                                                torch_dtype=self.dtype)\n",
    "        self.vae = vae\n",
    "\n",
    "        \n",
    "    def load_unet(self, unet_attn_slice=True):\n",
    "        \"\"\"Loads the U-Net.\n",
    "        \n",
    "        Optionally uses attention slicing to fit on smaller GPU cards.\n",
    "        \"\"\"\n",
    "        unet = UNet2DConditionModel.from_pretrained(\n",
    "            self.model_name,\n",
    "            subfolder=\"unet\",\n",
    "            torch_dtype=self.dtype)\n",
    "        # optionally enable unet attention slicing\n",
    "        if unet_attn_slice:\n",
    "            print('Enabling default unet attention slicing.')\n",
    "            slice_size = unet.config.attention_head_dim // 2\n",
    "            unet.set_attention_slice(slice_size)\n",
    "        self.unet = unet\n",
    "        \n",
    "                \n",
    "    def load_scheduler(self):\n",
    "        \"\"\"Loads the scheduler.\n",
    "        \"\"\"\n",
    "        scheduler = LMSDiscreteScheduler.from_pretrained(self.model_name, \n",
    "                                                         subfolder=\"scheduler\")\n",
    "        self.scheduler = scheduler \n",
    "\n",
    "\n",
    "    def generate(\n",
    "        self,\n",
    "        prompt,\n",
    "        guide_tfm=None,\n",
    "        width=512,\n",
    "        height=512,\n",
    "        steps=50,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"Main image generation loop.\n",
    "        \"\"\"\n",
    "        # if no guidance transform was given, use the default update\n",
    "        if guide_tfm is None:\n",
    "            print('Using the default Classifier-free Guidance.')\n",
    "            G = 7.5\n",
    "            guide_tfm = lambda u, t, idx: u + G*(t - u)\n",
    "        self.guide_tfm = guide_tfm\n",
    "        \n",
    "        # prepare the text embeddings\n",
    "        text = self.encode_text(prompt)\n",
    "        uncond = self.encode_text('')\n",
    "        text_emb = torch.cat([uncond, text]).type(self.unet.dtype)\n",
    "        \n",
    "        # start from the shared, initial latents\n",
    "        if getattr(self, 'init_latents', None) is None:\n",
    "            self.init_latents = self.get_initial_latents(height, width)\n",
    "            \n",
    "        latents = self.init_latents.clone().to(self.unet.device)\n",
    "        self.scheduler.set_timesteps(steps)\n",
    "        latents = latents * self.scheduler.init_noise_sigma\n",
    "        \n",
    "        # run the diffusion process\n",
    "        for i,ts in enumerate(tqdm(self.scheduler.timesteps)):\n",
    "            latents = self.diffuse_step(latents, text_emb, ts, i)\n",
    "\n",
    "        # decode the final latents and return the generated image\n",
    "        image = self.image_from_latents(latents)\n",
    "        return image    \n",
    "\n",
    "\n",
    "    def diffuse_step(self, latents, text_emb, ts, idx):\n",
    "        \"\"\"Runs a single diffusion step.\n",
    "        \"\"\"\n",
    "        inp = self.scheduler.scale_model_input(torch.cat([latents] * 2), ts)\n",
    "        with torch.no_grad(): \n",
    "            tf = ts\n",
    "            if torch.has_mps:\n",
    "                tf = ts.type(torch.float32)\n",
    "            preds = self.unet(inp, tf, encoder_hidden_states=text_emb)\n",
    "            u, t  = preds.sample.chunk(2)\n",
    "        \n",
    "        # run classifier-free guidance\n",
    "        pred = self.guide_tfm(u, t, idx)\n",
    "        \n",
    "        # update and return the latents\n",
    "        latents = self.scheduler.step(pred, ts, latents).prev_sample\n",
    "        return latents\n",
    "    \n",
    "\n",
    "    def encode_text(self, prompts, maxlen=None):\n",
    "        \"\"\"Extracts text embeddings from the given `prompts`.\n",
    "        \"\"\"\n",
    "        maxlen = maxlen or self.tokenizer.model_max_length\n",
    "        inp = self.tokenizer(prompts, padding=\"max_length\", max_length=maxlen, \n",
    "                             truncation=True, return_tensors=\"pt\")\n",
    "        inp_ids = inp.input_ids.to(self.device)\n",
    "        return self.text_encoder(inp_ids)[0]\n",
    "\n",
    "    \n",
    "    def to_device(self, device=None):\n",
    "        \"\"\"Places to pipeline pieces on the given device\n",
    "        \n",
    "        Note: assumes we keep Scheduler and Tokenizer on the cpu.\n",
    "        \"\"\"\n",
    "        device = device or self.device\n",
    "        for m in (self.text_encoder, self.vae, self.unet):\n",
    "            m.to(device)\n",
    "    \n",
    "    \n",
    "    def set_initial_latents(self, latents):\n",
    "        \"\"\"Sets the given `latents` as the initial noise latents.\n",
    "        \"\"\"\n",
    "        self.init_latents = latents\n",
    "        \n",
    "        \n",
    "    def get_initial_latents(self, height, width):\n",
    "        \"\"\"Returns \n",
    "        \"\"\"\n",
    "        return torch.randn((1, self.unet.in_channels, height//8, width//8),\n",
    "                           dtype=self.dtype, generator=self.generator)\n",
    "    \n",
    "    \n",
    "    def image_from_latents(self, latents):\n",
    "        \"\"\"Scales diffusion `latents` and turns them into a PIL Image.\n",
    "        \"\"\"\n",
    "        # scale and decode the latents\n",
    "        latents = 1 / 0.18215 * latents\n",
    "        with torch.no_grad():\n",
    "            data = self.vae.decode(latents).sample[0]\n",
    "        # Create PIL image\n",
    "        data = (data / 2 + 0.5).clamp(0, 1)\n",
    "        data = data.cpu().permute(1, 2, 0).float().numpy()\n",
    "        data = (data * 255).round().astype(\"uint8\")\n",
    "        image = Image.fromarray(data)\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
